{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SparkSession\n",
    "import pyspark.sql.functions\n",
    "from pyspark.ml.image import ImageSchema\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .appName(\"sql.functions tests\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE_LOCATION = '/Users/johnmorin/Documents/GitHub/Obj-Det-Tutorial/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images/train/*.jpg'\n",
    "LABEL_FILE_LOCATION = '/Users/johnmorin/Documents/GitHub/Obj-Det-Tutorial/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images/train_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = spark.read.format('image').load(TRAINING_FILE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in CSV attach\n",
    "label_files = spark.read.format('csv').option(\"header\", \"true\").load(LABEL_FILE_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- width: string (nullable = true)\n",
      " |-- height: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- xmin: string (nullable = true)\n",
      " |-- ymin: string (nullable = true)\n",
      " |-- xmax: string (nullable = true)\n",
      " |-- ymax: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_files.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = image_df.withColumn('filename', image_df.image.origin.cast(\"string\")[145:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cam_image26.jpg'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df.first().filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = image_df.join(label_files, image_df.filename == label_files.filename, 'inner').drop(image_df.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- width: string (nullable = true)\n",
      " |-- height: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- xmin: string (nullable = true)\n",
      " |-- ymin: string (nullable = true)\n",
      " |-- xmax: string (nullable = true)\n",
      " |-- ymax: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cam_image26.jpg'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.first().filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'nine':\n",
    "        return 1\n",
    "    elif row_label == 'ten':\n",
    "        return 2\n",
    "    elif row_label == 'jack':\n",
    "        return 3\n",
    "    elif row_label == 'queen':\n",
    "        return 4\n",
    "    elif row_label == 'king':\n",
    "        return 5\n",
    "    elif row_label == 'ace':\n",
    "        return 6\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "udf_labels = udf(class_text_to_int, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.withColumn('num_class', udf_labels(combined_df['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2 = combined_df.withColumn('img_data', combined_df['image'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2 = combined_df2.withColumn('origin', combined_df['image'].origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2 = combined_df2.drop('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- filename: string (nullable = true)\n",
      " |-- width: string (nullable = true)\n",
      " |-- height: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      " |-- xmin: string (nullable = true)\n",
      " |-- ymin: string (nullable = true)\n",
      " |-- xmax: string (nullable = true)\n",
      " |-- ymax: string (nullable = true)\n",
      " |-- num_class: integer (nullable = true)\n",
      " |-- img_data: binary (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "filenames_list = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_list = [str(x)[14:-2] for x in combined_df2.select('filename').distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in filenames_list:\n",
    "    combined_df2.where(combined_df2.filename==name).select('width')\n",
    "    combined_df2.where(combined_df2.filename==name).select('height')\n",
    "#    [list(row) for row in df.collect()]\n",
    "\n",
    "    x = [list(row) for row in combined_df2.where(combined_df2.filename==name).select('xmin').collect()]\n",
    "    combined_df2.where(combined_df2.filename==name).select('ymin').collect()\n",
    "    combined_df2.where(combined_df2.filename==name).select('xmax').collect()\n",
    "    combined_df2.where(combined_df2.filename==name).select('ymax').collect()\n",
    "    combined_df2.where(combined_df2.filename==name).select('num_class')\n",
    "    combined_df2.where(combined_df2.filename==name).select('class')\n",
    "    combined_df2.where(combined_df2.filename==name).select('img_data')\n",
    "    break\n",
    "    #print(combined_df2.where(col('filename')==name).select(col('xmin'))).collect()\n",
    "#blah.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['286']\n"
     ]
    }
   ],
   "source": [
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filenames_list2 = [str(i[0])[14:-2] for i in filenames_list]\n",
    "print(filenames_list2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'GroupedData' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-34b54cd8af11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mxmins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mxmaxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'GroupedData' object is not iterable"
     ]
    }
   ],
   "source": [
    "grouped = combined_df2.groupBy('filename')\n",
    "\n",
    "my_df = pd.DataFrame()\n",
    "\n",
    "for group in grouped:\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "    for i in range(0,group.count()-1):\n",
    "        im = group.take[i][0]\n",
    "        xmins.append(im.xmin / im.width)\n",
    "        xmaxs.append(im.xmax / im.width)\n",
    "        ymins.append(im.ymin / im.height)\n",
    "        ymaxs.append(im.ymax / im.height)\n",
    "#        classes_text.append(im.class)\n",
    "        # Changed to directly get class num from column\n",
    "        classes.append(im.num_class)\n",
    "    filename = group.first().filename\n",
    "    origin = group.first().origin\n",
    "    height = group.first().height\n",
    "    width = group.first().width\n",
    "    data = group.first().data\n",
    "    pd_data = pd.DataFrame({'image/height':height,'image/width':width, 'image/filename':filename, \n",
    "                  'image/encoded':data, 'image/format':'jpg', 'image/object/bbox/xmin':xmin,\n",
    "                  'image/object/bbox/ymin':ymin, 'image/object/bbox/xmax':xmax, 'image/object/bbox/ymax':ymax,\n",
    "                  'image/object/class/text':classes_text})\n",
    "    # , 'image/object/class/label':classes\n",
    "    bleh = pd.concat(my_df, pd_data)\n",
    "    \n",
    "fixed_df = createDataFrame(bleh)\n",
    "    #column_names = ['image/height', 'image/width', 'image/filename', 'image/encoded',\n",
    "    #                'image/format', 'image/object/bbox/xmin','image/object/bbox/ymin',\n",
    "    #                'image/object/bbox/xmax','image/object/bbox/ymax', 'image/object/class/text',\n",
    "    #                'image/object/class/label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pddf = combined_df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2.write.format('tfrecords').option('recordType', 'Example').save('tf_train.record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(group, path):\n",
    "    #with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        #encoded_jpg = fid.read()\n",
    "    #encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    #image = Image.open(encoded_jpg_io)\n",
    "    #width, height = image.size\n",
    "    height = group.height[0]\n",
    "    width = group.width[0]\n",
    "    encoded_jpg = group.img_data[0]\n",
    "    \n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        # Changed to directly get class num from column\n",
    "        classes.append(row['num_class'])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Need to alter for .ipynb\n",
    "\n",
    "def main(_):\n",
    "    #writer = tf.python_io.TFRecordWriter(FLAGS.output_path)\n",
    "    #path = os.path.join(FLAGS.image_dir)\n",
    "    #examples = pd.read_csv(FLAGS.csv_input)\n",
    "    # changed to Pandas Version of Combined_DF from above DataFrame\n",
    "    grouped = split(pddf, 'filename')\n",
    "    for group in grouped:\n",
    "        tf_example = create_tf_example(group, path)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n",
    "    print('Successfully created the TFRecords: {}'.format(output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Potentially Faster way to run labels?\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def pandas_text_to_int(row_label):\n",
    "    if row_label is 'nine':\n",
    "        return 1\n",
    "    elif row_label is 'ten':\n",
    "        return 2\n",
    "    elif row_label is 'jack':\n",
    "        return 3\n",
    "    elif row_label is 'queen':\n",
    "        return 4\n",
    "    elif row_label is 'king':\n",
    "        return 5\n",
    "    elif row_label is 'ace':\n",
    "        return 6\n",
    "    #else:\n",
    "        #return None\n",
    "        \n",
    "blah = combined_df.withColumn('num_class', pandas_text_to_int(combined_df['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.take(2)    # take the first four rows of the dataframe\n",
    "\n",
    "for i in range(0,2):\n",
    "    im = data[i][0]\n",
    "    print(\"image index: {}\".format(i))\n",
    "    print(\"image type: {}, number of fields: {}\".format(type(im), len(im)))\n",
    "    print(\"image path: {}\".format(im.origin))\n",
    "    print(\"height: {}, width: {}, OpenCV type: {}\".format(im.height, im.width, im.mode))\n",
    "    print(\"nChannels: {}\".format(im.nChannels))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
