{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, ArrayType\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .appName(\"sql.functions tests\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiff_train_loc = '/Users/johnmorin/KaylaTek/Data/images/train/*.tif'\n",
    "tiff_test_loc = '/Users/johnmorin/KaylaTek/Data/images/test/*.tif'\n",
    "tiff_train_labels = '/Users/johnmorin/KaylaTek/Data/images/training_data.csv'\n",
    "tiff_test_labels = '/Users/johnmorin/KaylaTek/Data/images/test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_file_loc = '/Users/johnmorin/Documents/GitHub/Obj-Det-Tutorial/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images/train_labels.csv'\n",
    "#test_file_loc = '/Users/johnmorin/Documents/GitHub/Obj-Det-Tutorial/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images/test_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING_FILE_LOCATION = '/Users/johnmorin/Documents/GitHub/Obj-Det-Tutorial/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images/train/*.jpg'\n",
    "#TEST_FILE_LOCATION = '/Users/johnmorin/Documents/GitHub/Obj-Det-Tutorial/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10/images/train/*.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tif_image_df = spark.read.format('image').load('/Users/johnmorin/KaylaTek/Data/images/train/frame_000000.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to_shape = tf.cast(tf.stack([1024, 1280, 3]), tf.int32)\n",
    "#image = tf.reshape(tf.decode_raw(encoded_inputs, tf.uint8), to_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'nine':\n",
    "        return 1\n",
    "    elif row_label == 'ten':\n",
    "        return 2\n",
    "    elif row_label == 'jack':\n",
    "        return 3\n",
    "    elif row_label == 'queen':\n",
    "        return 4\n",
    "    elif row_label == 'king':\n",
    "        return 5\n",
    "    elif row_label == 'ace':\n",
    "        return 6\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label_files(label_filename):\n",
    "    df = pd.read_csv(label_filename)\n",
    "    grouped = df['filename'].unique()\n",
    "    \n",
    "    csv_df = pd.DataFrame()\n",
    "    for files in grouped:\n",
    "        mydf = df[df['filename'] == files]\n",
    "        #print(mydf.head())\n",
    "        height = int(mydf['h'].unique()[0])\n",
    "        width = int(mydf['w'].unique()[0])\n",
    "        filename = mydf.filename.unique()[0]\n",
    "        num_label = mydf['classes'].tolist()\n",
    "        label = mydf['classes_text'].tolist()\n",
    "        #print(list(label))\n",
    "        #num_label = mydf['class'].map(class_text_to_int).tolist()\n",
    "        # Creating proportional locations for the bounding boxes instead of pixel values\n",
    "        xmin = mydf['xmin'].map(lambda x: x/width).tolist()\n",
    "        ymin = mydf['ymin'].map(lambda x: x/height).tolist()\n",
    "        xmax = mydf['xmax'].map(lambda x: x/width).tolist()\n",
    "        ymax = mydf['ymax'].map(lambda x: x/height).tolist()\n",
    "        temp = pd.DataFrame({'image/filename':filename, 'image/width':width, 'image/height':height, \n",
    "                        'image/object/class/label':[num_label], 'image/object/bbox/xmin':[xmin], \n",
    "                        'image/object/bbox/ymin':[ymin],'image/object/bbox/xmax':[xmax],\n",
    "                        'image/object/bbox/ymax':[ymax], 'image/object/class/text':[label],\n",
    "                        # Change Number of Channels to correct number\n",
    "                        'image/channels': int(3)\n",
    "                        #'image/encoded': dataset_util.bytes_feature(encoded_inputs),\n",
    "                        })\n",
    "        csv_df = pd.concat([csv_df, temp])\n",
    "    csv_df.reset_index(inplace= True)\n",
    "    csv_df.drop('index', axis=1, inplace = True)\n",
    "    csv_df['image/format'] = '.tif'\n",
    "    return csv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenCV to read in binary for image file\n",
    "#image_df = image_df.withColumn('file_locs', image_df.image.origin.substr(6, 300))\n",
    "#cv2_udf = udf(lambda x: cv2.imread(x, 1).tostring(), BinaryType())\n",
    "#image_df = image_df.withColumn('cv2', cv2_udf(image_df.file_locs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_df = spark.read.format('image').load(tiff_train_loc)\n",
    "image_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/Users/johnmorin/KaylaTek/Data/images/train/frame_000365.tif'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df.first().image.origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecord(pandas_df, tf_record_loc, image_location, which):\n",
    "    pand_df = sqlContext.createDataFrame(pandas_df)\n",
    "    image_df = spark.read.format('image').load(image_location)\n",
    "    if which is 'train':\n",
    "        image_df = image_df.withColumn('filename', image_df.image.origin.substr(50, 300))\n",
    "    if which is 'test':\n",
    "        image_df = image_df.withColumn('filename', image_df.image.origin.substr(49, 300))\n",
    "    combined_df = image_df.join(pand_df, \\\n",
    "                            image_df.filename == pand_df['image/filename'], \\\n",
    "                            'inner').drop(image_df.filename)\n",
    "    # Original Code to add encoded data to \n",
    "    #combined_df = combined_df.withColumn('image/encoded', combined_df['image'].data)\n",
    "    # Code to encode tiffs into binary format when pyspark won't read them\n",
    "    combined_df = combined_df.withColumn('image/source_id', combined_df.image.origin.substr(6, 300))\n",
    "    cv2_udf = udf(lambda x: cv2.imread(x, 1).tostring(), BinaryType())\n",
    "    combined_df = combined_df.withColumn('image/encoded', cv2_udf(combined_df['image/source_id']))\n",
    "    #print(combined_df.first())\n",
    "    # Cleaning up dataframe before writing it to tfrecord\n",
    "    combined_df = combined_df.drop('image')\n",
    "    \n",
    "    # Writing Pyspark Dataframe to tfrecord\n",
    "    combined_df.write.format('tfrecords').option('recordType', 'Example').save(tf_record_loc)\n",
    "    #combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_label_files(tiff_train_labels)\n",
    "test_df = load_label_files(tiff_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(image=Row(origin='file:/Users/johnmorin/KaylaTek/Data/images/train/frame_000365.tif', height=-1, width=-1, nChannels=-1, mode=-1, data=bytearray(b'')), filename='frame_000365.tif')\n",
      "Row(image=Row(origin='file:/Users/johnmorin/KaylaTek/Data/images/test/frame_000403.tif', height=-1, width=-1, nChannels=-1, mode=-1, data=bytearray(b'')), filename='frame_000403.tif')\n"
     ]
    }
   ],
   "source": [
    "create_tfrecord(train_df, 'tf_tiff_train', tiff_train_loc, 'train')\n",
    "create_tfrecord(test_df, 'tf_tiff_test', tiff_test_loc, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_files = spark.read.format('csv') \\\n",
    "#    .option(\"header\", \"true\") \\\n",
    "#    .option(\"inferSchema\", 'true') \\\n",
    "#    .load('train_label_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords = '/tf_card_train/*.*'\n",
    "tf_single = 'pand_tf_train.record/part-r-00000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3033.load.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern file:/tf_card_train/*.* matches 0 files\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:387)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorFlowInferSchema$.apply(TensorFlowInferSchema.scala:39)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation$$anonfun$3.apply(TensorflowRelation.scala:42)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation$$anonfun$3.apply(TensorflowRelation.scala:42)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation.x$1$lzycompute(TensorflowRelation.scala:42)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation.x$1(TensorflowRelation.scala:32)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation.tfSchema$lzycompute(TensorflowRelation.scala:32)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation.tfSchema(TensorflowRelation.scala:32)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation.schema(TensorflowRelation.scala:59)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:403)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-d9692b64110a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf_rec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfrecords\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recordType\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Example\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3033.load.\n: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern file:/tf_card_train/*.* matches 0 files\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:387)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorFlowInferSchema$.apply(TensorFlowInferSchema.scala:39)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation$$anonfun$3.apply(TensorflowRelation.scala:42)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation$$anonfun$3.apply(TensorflowRelation.scala:42)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation.x$1$lzycompute(TensorflowRelation.scala:42)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation.x$1(TensorflowRelation.scala:32)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation.tfSchema$lzycompute(TensorflowRelation.scala:32)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation.tfSchema(TensorflowRelation.scala:32)\n\tat org.tensorflow.spark.datasources.tfrecords.TensorflowRelation.schema(TensorflowRelation.scala:59)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:403)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "tf_rec = spark.read.format(\"tfrecords\").option(\"recordType\", \"Example\").load(tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image/object/class/text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- image/width: long (nullable = true)\n",
      " |-- image/height: long (nullable = true)\n",
      " |-- image/object/bbox/ymax: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- image/object/bbox/xmin: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- image/filename: string (nullable = true)\n",
      " |-- image/format: string (nullable = true)\n",
      " |-- image/source_id: string (nullable = true)\n",
      " |-- image/object/class/label: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- image/encoded: string (nullable = true)\n",
      " |-- image/object/bbox/xmax: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- image/object/bbox/ymin: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_rec.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
